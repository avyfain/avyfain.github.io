---
title: "Everything And More, a short review"
main_image: napa/previews/3.jpg
layout: post
tags: [books, review, mathematics, machine learning, history]
description: "A book about the history of mathematics made me think of the future of computer science."
category: articles
---

The first time I heard about David Foster Wallace was during my sophomore year of college. My girlfriend was powering through _Infinite Jest_, and was surprised I had never heard about the book or its author. I told her it sounded unbearable. A few years later, a friend picked [The Weasel, Twelve Monkeys and the Shrub](https://www.rollingstone.com/politics/features/david-foster-wallace-on-john-mccain-2000-rolling-stone-story-w493273) as reading material for our article club[^1], and I had no choice - I read it and hated it. Then this year, I was talking about [GEB](https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach) with my boss's boss's boss and he insisted that even if I hated DFW's writing, I had to read his book on infinity. I followed his advice.

_Everything and More_ walks a thin line between being too technical for the untrained reader and too dumbed down for the expert. For better or for worse, I fall somewhere in between. I enjoyed reading about Mr. Chicken, the rooster who _inductively_ figured out that _t_(man + sack)=food, only to end up dead in the sack himself a few weeks later. I also enjoyed figuring out the inner workings of $$\aleph _{0}$$ and drudging through proofs of how certain kinds of infinities are greater than others. As someone without a pure math background, it seems like DFW did a good job of exposing the reader to a wide set of ideas. How thoroughly he explains them is a different issue. From some quick research it seems like the book got a lot of well-deserved flak for it's accuracy (or lack thereof) upon release. Regardless, it is worth reading not so much for its comprehensive proofs, but for the way it illuminates the development of modern mathematics.

Early on, DFW explains that, in math, paradoxes arise from expressing logical systems as natural language. For example, halve the distance between two sidewalks enough times, and with a bit of rhetoric and pomp even the smartest people start to believe that [crossing a street is impossible](https://en.wikipedia.org/wiki/Zeno%27s_paradoxes). Thanks, infinite regress! The whole book is peppered with the histories of these paradoxes, and of mathematicians trying to overcome them. From the Greeks to Galileo, over time there is a growing gap between the theorists (dealing in abstraction) and the engineers and physicists (dealing with reality). The two can't be easily reconciled, but that only concerns one of the two groups. The issue is especially exacerbated with the discovery of calculus in the 17th century, and the explotion of its applications in construction, warfare, navigation, astronomy, etc:

> The situation of mathematics after 1700 is intensely weird [...] mathematical discoveries enabled scientific advances, which themselves motivated further math discoveries. [This] created for math a situation that resembled a tree with great lush proliferant systems of branches but no real roots. There were still no grounded, rigorous definitions of the differential, derivative, integral, limit, or convergent/divergent series. Not even of the function. There was constant controversy, and yet at the same time nobody seemed to care.
>
> David Foster Wallace - Everything and More, ยง5a

For roughly a century and a half people solved a panoply of life-changing human problems via calculus, without understanding its underpinnings. Later on, the book compares this disparity to a market bubble. The appearance of analysis, set theory, and modern mathematics, as explained by DFW, are the response to this chaos, set in motion by the discovery of calculus. The rigor that these new branches of math brought to the field eventually became the foundation that powered the technological advances of the 19th and early 20th century.

What is most striking is how much of a parallel we can draw between the development of calculus and today's state of the art in computer science, with machine learning being particularly pertinent - the whole field is based on empiricism and induction. The eighteenth and nineteenth century engineers treated the methods of calculus as black boxes, and trusted them enough to use their answers to construct buildings, to predict the movement of stars, and to sail across the Atlantic. _Whole complex human systems based on the output of black boxes._ Read that again and tell me that there isn't a parallel to software engineering and machine learning. It makes one wonder where our technology today might be headed.

Notice that this is not a dis on the Bengios and Hintons of the world, nor on my peers who are practitioners in the field. To the contrary. If anything its a comparison to Newton and Leibniz kicking off this transformation, and to the Boyles and the Watts who put the knowledge in motion.

What DFW's book in a sense revealed to me is that while today anyone can download Tensorflow and feed the black box with some data to solve real problems, there's a lot still to uncover. The frontier of knowledge is ever expanding, and it has not disappointed us so far. Perhaps all we need is our own Karl Weierstrass to come and wake us up from the stupor, and a George Cantor to lead the way forward. Computer science is as alive as ever, and for the last fifty years has driven the greatest technological revolution in history. Just imagine where we'll be when we move past the black box.

[^1]: Article club? That's a story for another time.

<hr>
<small><em>Photo: by me, previously posted on [Napa](photos/2016/10/31/napa/).</em></small>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
